{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ+kvHl9Jlsnf8Tqn7nvBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snigdha2808/Gen-AI-/blob/main/Build_own_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU9P8831MS82",
        "outputId": "10258c81-319b-44ba-a499-44b864c43687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Set Up\n",
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A language model predicts the next word in a sentence.\n",
        "# We’ll keep things simple and build a bigram model. This just means that our\n",
        "# model will predict the next word using only the current word.\n",
        "import numpy as np\n",
        "\n",
        "# Sample dataset: A small text corpus\n",
        "corpus = \"\"\"Artificial Intelligence is the new electricity.\n",
        "Machine learning is the future of AI.\n",
        "AI is transforming industries and shaping the future.\"\"\"\n",
        "\n",
        "# Preparing the Text\n",
        "# First things first, we need to break this text into individual words and\n",
        "# create a vocabulary (basically a list of all unique words). This gives us\n",
        "# something to work with. Tokenize the corpus into words\n",
        "words = corpus.lower().split()\n",
        "\n",
        "# Create a vocabulary of unique words\n",
        "vocab = list(set(words))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary: {vocab}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhh0pErPMl_h",
        "outputId": "983094eb-6b82-4d0b-c28f-632607911072"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['learning', 'and', 'ai', 'artificial', 'ai.', 'transforming', 'is', 'industries', 'electricity.', 'new', 'the', 'future', 'shaping', 'future.', 'intelligence', 'machine', 'of']\n",
            "Vocabulary size: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map Words to Numbers\n",
        "# Computers work with numbers, not words. So, we’ll map each word to an index\n",
        "# and create a reverse mapping too (this will help when we convert them back to\n",
        "# words later).\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)} #Dictionary comprehension- reducing the concept\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "# Convert the words in the corpus to indices\n",
        "corpus_indices = [word_to_idx[word] for word in words]\n",
        "\n",
        "# we’re just turning words into numbers that our model can understand. Each word gets its own number,\n",
        "# like “AI” might become 0, and “learning” might become 1, depending on the order.\n"
      ],
      "metadata": {
        "id": "TcEp0dXRM_Rd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Model\n",
        "# Now, let’s get to the heart of it: building the bigram model.\n",
        "# We want to figure out the probability of one word following another. To do\n",
        "# that, we’ll count how often each word pair (bigram) shows up in our dataset.\n",
        "# Initialize bigram counts matrix\n",
        "bigram_counts = np.zeros((vocab_size, vocab_size))\n",
        "\n",
        "# Count occurrences of each bigram in the corpus\n",
        "for i in range(len(corpus_indices) - 1):\n",
        "    current_word = corpus_indices[i]\n",
        "    next_word = corpus_indices[i + 1]\n",
        "    bigram_counts[current_word, next_word] += 1\n",
        "\n",
        "# Apply Laplace smoothing by adding 1 to all bigram counts\n",
        "bigram_counts += 0.01\n",
        "\n",
        "# Normalize the counts to get probabilities\n",
        "bigram_probabilities = bigram_counts / bigram_counts.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(\"Bigram probabilities matrix: \", bigram_probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snpDOr0cNKMc",
        "outputId": "7f6aa9d8-8bb0-4259-9976-f3d7bb365e50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram probabilities matrix:  [[0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.86324786 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.86324786 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.86324786 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.86324786 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.86324786 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.86324786 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00315457 0.00315457 0.00315457 0.00315457 0.00315457 0.31861199\n",
            "  0.00315457 0.00315457 0.00315457 0.00315457 0.6340694  0.00315457\n",
            "  0.00315457 0.00315457 0.00315457 0.00315457 0.00315457]\n",
            " [0.00854701 0.86324786 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.86324786 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.86324786 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00315457 0.00315457 0.00315457 0.00315457 0.00315457 0.00315457\n",
            "  0.00315457 0.00315457 0.00315457 0.31861199 0.00315457 0.31861199\n",
            "  0.00315457 0.31861199 0.00315457 0.00315457 0.00315457]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.86324786]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.86324786 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.05882353 0.05882353 0.05882353 0.05882353 0.05882353 0.05882353\n",
            "  0.05882353 0.05882353 0.05882353 0.05882353 0.05882353 0.05882353\n",
            "  0.05882353 0.05882353 0.05882353 0.05882353 0.05882353]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.86324786 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.86324786 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]\n",
            " [0.00854701 0.00854701 0.00854701 0.00854701 0.86324786 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701 0.00854701\n",
            "  0.00854701 0.00854701 0.00854701 0.00854701 0.00854701]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the Next Word\n",
        "\n",
        "def predict_next_word(current_word, bigram_probabilities):\n",
        "    word_idx = word_to_idx[current_word]\n",
        "    next_word_probs = bigram_probabilities[word_idx]\n",
        "    next_word_idx = np.random.choice(range(vocab_size), p=next_word_probs)\n",
        "    return idx_to_word[next_word_idx]\n",
        "\n",
        "# Test the model with a word\n",
        "current_word = \"ai\"\n",
        "next_word = predict_next_word(current_word, bigram_probabilities)\n",
        "print(f\"Given '{current_word}', the model predicts '{next_word}'.\")\n",
        "\n",
        "# This function takes a word, looks up its probabilities, and randomly selects\n",
        "# the next word based on those probabilities. If you pass in \"AI,\" the model\n",
        "# might predict something like \"is\" as the next word."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP_xTxOQOmzJ",
        "outputId": "c8c774c6-d967-4b57-d8f9-d22c28bae40e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given 'ai', the model predicts 'is'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(start_word, bigram_probabilities, length=5):\n",
        "    sentence = [start_word]\n",
        "    current_word = start_word\n",
        "\n",
        "    for _ in range(length):\n",
        "        next_word = predict_next_word(current_word, bigram_probabilities)\n",
        "        sentence.append(next_word)\n",
        "        current_word = next_word\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Generate a sentence starting with \"artificial\"\n",
        "generated_sentence = generate_sentence(\"artificial\", bigram_probabilities, length=10)\n",
        "print(f\"Generated sentence: {generated_sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Y9rykZOwj1",
        "outputId": "b903c76e-170c-4615-c555-db250364a6f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentence: artificial intelligence is the future. learning future. is transforming industries and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f18de26"
      },
      "source": [
        "To link this notebook with your GitHub repository, you can save a copy to GitHub directly from Colab. Go to **File** > **Save a copy to GitHub**."
      ]
    }
  ]
}